{"version":3,"sources":["../src/actions/describe-image.ts","../src/templates.ts","../src/types.ts","../src/services/llama.ts","../src/index.ts"],"sourcesContent":["import {\n    type Action,\n    type IAgentRuntime,\n    type Memory,\n    type State,\n    type HandlerCallback,\n    composeContext,\n    generateObject,\n    type ActionExample,\n    ModelClass,\n    elizaLogger,\n    ServiceType,\n    type IImageDescriptionService,\n} from \"@elizaos/core\";\nimport { getFileLocationTemplate } from \"../templates\";\nimport { FileLocationResultSchema, isFileLocationResult } from \"../types\";\n\nexport const describeImage: Action = {\n    name: \"DESCRIBE_IMAGE\",\n    similes: [\"DESCRIBE_PICTURE\", \"EXPLAIN_PICTURE\", \"EXPLAIN_IMAGE\"],\n    validate: async (_runtime: IAgentRuntime, _message: Memory) => {\n        return true;\n    },\n    description: \"Describe an image\",\n    handler: async (\n        runtime: IAgentRuntime,\n        message: Memory,\n        state: State,\n        _options: { [key: string]: unknown },\n        callback?: HandlerCallback\n    ): Promise<boolean> => {\n        // Create context with attachments and URL\n        const getFileLocationContext = composeContext({\n            state,\n            template: getFileLocationTemplate,\n        });\n\n        const fileLocationResultObject = await generateObject({\n            runtime,\n            context: getFileLocationContext,\n            modelClass: ModelClass.SMALL,\n            schema: FileLocationResultSchema,\n            stop: [\"\\n\"],\n        });\n\n        if (\n            !isFileLocationResult(\n                fileLocationResultObject?.object ?? fileLocationResultObject\n            )\n        ) {\n            elizaLogger.error(\"Failed to generate file location\");\n            return false;\n        }\n\n        let fileLocation = (fileLocationResultObject?.object as any)\n            ?.fileLocation;\n        fileLocation ??= fileLocationResultObject;\n\n        const { description } = await runtime\n            .getService<IImageDescriptionService>(ServiceType.IMAGE_DESCRIPTION)\n            .describeImage(fileLocation);\n\n        runtime.messageManager.createMemory({\n            userId: message.agentId,\n            agentId: message.agentId,\n            roomId: message.roomId,\n            content: {\n                text: description,\n            },\n        });\n\n        callback({\n            text: description,\n        });\n\n        return true;\n    },\n    examples: [\n        [\n            {\n                user: \"{{user1}}\",\n                content: {\n                    text: \"Can you describe this image for me?\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"Let me analyze this image for you...\",\n                    action: \"DESCRIBE_IMAGE\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"I see an orange tabby cat sitting on a windowsill. The cat appears to be relaxed and looking out the window at birds flying by. The lighting suggests it's a sunny afternoon.\",\n                },\n            },\n        ],\n        [\n            {\n                user: \"{{user1}}\",\n                content: {\n                    text: \"What's in this picture?\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"I'll take a look at that image...\",\n                    action: \"DESCRIBE_IMAGE\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"The image shows a modern kitchen with stainless steel appliances. There's a large island counter in the center with marble countertops. The cabinets are white with sleek handles, and there's pendant lighting hanging above the island.\",\n                },\n            },\n        ],\n        [\n            {\n                user: \"{{user1}}\",\n                content: {\n                    text: \"Could you tell me what this image depicts?\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"I'll describe this image for you...\",\n                    action: \"DESCRIBE_IMAGE\",\n                },\n            },\n            {\n                user: \"{{user2}}\",\n                content: {\n                    text: \"This is a scenic mountain landscape at sunset. The peaks are snow-capped and reflected in a calm lake below. The sky is painted in vibrant oranges and purples, with a few wispy clouds catching the last rays of sunlight.\",\n                },\n            },\n        ],\n    ] as ActionExample[][],\n} as Action;\n","export const getFileLocationTemplate = `\n{{recentMessages}}\n\nextract the file location from the users message or the attachment in the message history that they are referring to.\nyour job is to infer the correct attachment based on the recent messages, the users most recent message, and the attachments in the message\nimage attachments are the result of the users uploads, or images you have created.\nonly respond with the file location, no other text.\ntypically the file location is in the form of a URL or a file path.\n\n\\`\\`\\`json\n{\n    \"fileLocation\": \"file location text goes here\"\n}\n\\`\\`\\`\n`;\n","import { z } from \"zod\";\n\nexport const FileLocationResultSchema = z.object({\n    fileLocation: z.string().min(1),\n});\n\nexport type FileLocationResult = z.infer<typeof FileLocationResultSchema>;\n\nexport function isFileLocationResult(obj: unknown): obj is FileLocationResult {\n    return FileLocationResultSchema.safeParse(obj).success;\n}\n","import { elizaLogger, type IAgentRuntime, ServiceType, ModelProviderName } from \"@elizaos/core\";\nimport { Service } from \"@elizaos/core\";\nimport fs from \"fs\";\nimport https from \"https\";\nimport { type GbnfJsonSchema, getLlama, type Llama, LlamaChatSession, type LlamaChatSessionRepeatPenalty, type LlamaContext, type LlamaContextSequence, type LlamaContextSequenceRepeatPenalty, LlamaJsonSchemaGrammar, type LlamaModel, type Token } from \"node-llama-cpp\";\nimport path from \"path\";\nimport si from \"systeminformation\";\nimport { fileURLToPath } from \"url\";\n\nconst wordsToPunish = [\n  \" please\",\n  \" feel\",\n  \" free\",\n  \"!\",\n  \"–\",\n  \"—\",\n  \"?\",\n  \".\",\n  \",\",\n  \"; \",\n  \" cosmos\",\n  \" tapestry\",\n  \" tapestries\",\n  \" glitch\",\n  \" matrix\",\n  \" cyberspace\",\n  \" troll\",\n  \" questions\",\n  \" topics\",\n  \" discuss\",\n  \" basically\",\n  \" simulation\",\n  \" simulate\",\n  \" universe\",\n  \" like\",\n  \" debug\",\n  \" debugging\",\n  \" wild\",\n  \" existential\",\n  \" juicy\",\n  \" circuits\",\n  \" help\",\n  \" ask\",\n  \" happy\",\n  \" just\",\n  \" cosmic\",\n  \" cool\",\n  \" joke\",\n  \" punchline\",\n  \" fancy\",\n  \" glad\",\n  \" assist\",\n  \" algorithm\",\n  \" Indeed\",\n  \" Furthermore\",\n  \" However\",\n  \" Notably\",\n  \" Therefore\",\n  \" Additionally\",\n  \" conclusion\",\n  \" Significantly\",\n  \" Consequently\",\n  \" Thus\",\n  \" What\",\n  \" Otherwise\",\n  \" Moreover\",\n  \" Subsequently\",\n  \" Accordingly\",\n  \" Unlock\",\n  \" Unleash\",\n  \" buckle\",\n  \" pave\",\n  \" forefront\",\n  \" harness\",\n  \" harnessing\",\n  \" bridging\",\n  \" bridging\",\n  \" Spearhead\",\n  \" spearheading\",\n  \" Foster\",\n  \" foster\",\n  \" environmental\",\n  \" impact\",\n  \" Navigate\",\n  \" navigating\",\n  \" challenges\",\n  \" chaos\",\n  \" social\",\n  \" inclusion\",\n  \" inclusive\",\n  \" diversity\",\n  \" diverse\",\n  \" delve\",\n  \" noise\",\n  \" infinite\",\n  \" insanity\",\n  \" coffee\",\n  \" singularity\",\n  \" AI\",\n  \" digital\",\n  \" artificial\",\n  \" intelligence\",\n  \" consciousness\",\n  \" reality\",\n  \" metaverse\",\n  \" virtual\",\n  \" virtual reality\",\n  \" VR\",\n  \" Metaverse\",\n  \" humanity\",\n];\n\nconst __dirname = path.dirname(fileURLToPath(import.meta.url));\n\nconst jsonSchemaGrammar: Readonly<{\n  type: string;\n  properties: {\n    user: {\n      type: string;\n    };\n    content: {\n      type: string;\n    };\n  };\n}> = {\n  type: \"object\",\n  properties: {\n    user: {\n      type: \"string\",\n    },\n    content: {\n      type: \"string\",\n    },\n  },\n};\n\ninterface QueuedMessage {\n  context: string;\n  temperature: number;\n  stop: string[];\n  max_tokens: number;\n  frequency_penalty: number;\n  presence_penalty: number;\n  useGrammar: boolean;\n  resolve: (value: any | string | PromiseLike<any | string>) => void;\n  reject: (reason?: any) => void;\n}\n\nexport class LlamaService extends Service {\n  private llama: Llama | undefined;\n  private model: LlamaModel | undefined;\n  private modelPath: string;\n  private grammar: LlamaJsonSchemaGrammar<GbnfJsonSchema> | undefined;\n  private ctx: LlamaContext | undefined;\n  private sequence: LlamaContextSequence | undefined;\n  private modelUrl: string;\n  private ollamaModel: string | undefined;\n\n  private messageQueue: QueuedMessage[] = [];\n  private isProcessing = false;\n  private modelInitialized = false;\n  private runtime: IAgentRuntime | undefined;\n\n  static serviceType: ServiceType = ServiceType.TEXT_GENERATION;\n\n  constructor() {\n    super();\n    this.llama = undefined;\n    this.model = undefined;\n    this.modelUrl = \"https://huggingface.co/NousResearch/Hermes-3-Llama-3.1-8B-GGUF/resolve/main/Hermes-3-Llama-3.1-8B.Q8_0.gguf?download=true\";\n    \n  }\n\n  async initialize(runtime: IAgentRuntime): Promise<void> {\n    const modelName = \"model.gguf\";\n    elizaLogger.info(\"Initializing LlamaService...\");\n    this.runtime = runtime;\n    this.modelPath = path.join(runtime.getSetting(\"LLAMALOCAL_PATH\").trim() ?? \"./\", modelName);\n    this.ollamaModel = runtime.getSetting(\"OLLAMA_MODEL\");\n  }\n\n  private async ensureInitialized() {\n    if (!this.modelInitialized) {\n      elizaLogger.info(\"Model not initialized, starting initialization...\");\n      await this.initializeModel();\n    } else {\n      elizaLogger.info(\"Model already initialized\");\n    }\n  }\n\n  async initializeModel() {\n    try {\n      elizaLogger.info(\"Checking model file...\");\n      await this.checkModel();\n\n      const systemInfo = await si.graphics();\n      const hasCUDA = systemInfo.controllers.some((controller) => controller.vendor.toLowerCase().includes(\"nvidia\"));\n\n      if (hasCUDA) {\n        elizaLogger.info(\"LlamaService: CUDA detected, using GPU acceleration\");\n      } else {\n        elizaLogger.warn(\"LlamaService: No CUDA detected - local response will be slow\");\n      }\n\n      elizaLogger.info(\"Initializing Llama instance...\");\n      this.llama = await getLlama({\n        gpu: hasCUDA ? \"cuda\" : undefined,\n      });\n\n      elizaLogger.info(\"Creating JSON schema grammar...\");\n      const grammar = new LlamaJsonSchemaGrammar(this.llama, jsonSchemaGrammar as GbnfJsonSchema);\n      this.grammar = grammar;\n\n      elizaLogger.info(\"Loading model...\");\n      this.model = await this.llama.loadModel({\n        modelPath: this.modelPath,\n      });\n\n      elizaLogger.info(\"Creating context and sequence...\");\n      this.ctx = await this.model.createContext({ contextSize: 8192 });\n      this.sequence = this.ctx.getSequence();\n\n      this.modelInitialized = true;\n      elizaLogger.success(\"Model initialization complete\");\n      this.processQueue();\n    } catch (error) {\n      elizaLogger.error(\"Model initialization failed. Deleting model and retrying:\", error);\n      try {\n        elizaLogger.info(\"Attempting to delete and re-download model...\");\n        await this.deleteModel();\n        await this.initializeModel();\n      } catch (retryError) {\n        elizaLogger.error(\"Model re-initialization failed:\", retryError);\n        throw new Error(`Model initialization failed after retry: ${retryError.message}`);\n      }\n    }\n  }\n\n  async checkModel() {\n    if (!fs.existsSync(this.modelPath)) {\n      elizaLogger.info(\"Model file not found, starting download...\");\n      await new Promise<void>((resolve, reject) => {\n        const file = fs.createWriteStream(this.modelPath);\n        let downloadedSize = 0;\n        let totalSize = 0;\n\n        const downloadModel = (url: string) => {\n          https\n            .get(url, (response) => {\n              if (response.statusCode >= 300 && response.statusCode < 400 && response.headers.location) {\n                elizaLogger.info(`Following redirect to: ${response.headers.location}`);\n                downloadModel(response.headers.location);\n                return;\n              }\n\n              if (response.statusCode !== 200) {\n                reject(new Error(`Failed to download model: HTTP ${response.statusCode}`));\n                return;\n              }\n\n              totalSize = Number.parseInt(response.headers[\"content-length\"] || \"0\", 10);\n              elizaLogger.info(`Downloading model: Hermes-3-Llama-3.1-8B.Q8_0.gguf`);\n              elizaLogger.info(`Download location: ${this.modelPath}`);\n              elizaLogger.info(`Total size: ${(totalSize / 1024 / 1024).toFixed(2)} MB`);\n\n              response.pipe(file);\n\n              let progressString = \"\";\n              response.on(\"data\", (chunk) => {\n                downloadedSize += chunk.length;\n                const progress = totalSize > 0 ? ((downloadedSize / totalSize) * 100).toFixed(1) : \"0.0\";\n                const dots = \".\".repeat(Math.floor(Number(progress) / 5));\n                progressString = `Downloading model: [${dots.padEnd(20, \" \")}] ${progress}%`;\n                elizaLogger.progress(progressString);\n              });\n\n              file.on(\"finish\", () => {\n                file.close();\n                elizaLogger.progress(\"\"); // Clear the progress line\n                elizaLogger.success(\"Model download complete\");\n                resolve();\n              });\n\n              response.on(\"error\", (error) => {\n                fs.unlink(this.modelPath, () => {});\n                reject(new Error(`Model download failed: ${error.message}`));\n              });\n            })\n            .on(\"error\", (error) => {\n              fs.unlink(this.modelPath, () => {});\n              reject(new Error(`Model download request failed: ${error.message}`));\n            });\n        };\n\n        downloadModel(this.modelUrl);\n\n        file.on(\"error\", (err) => {\n          fs.unlink(this.modelPath, () => {}); // Delete the file async\n          console.error(\"File write error:\", err.message);\n          reject(err);\n        });\n      });\n    } else {\n      elizaLogger.warn(\"Model already exists.\");\n    }\n  }\n\n  async deleteModel() {\n    if (fs.existsSync(this.modelPath)) {\n      fs.unlinkSync(this.modelPath);\n    }\n  }\n\n  async queueMessageCompletion(context: string, temperature: number, stop: string[], frequency_penalty: number, presence_penalty: number, max_tokens: number): Promise<any> {\n    await this.ensureInitialized();\n    return new Promise((resolve, reject) => {\n      this.messageQueue.push({\n        context,\n        temperature,\n        stop,\n        frequency_penalty,\n        presence_penalty,\n        max_tokens,\n        useGrammar: true,\n        resolve,\n        reject,\n      });\n      this.processQueue();\n    });\n  }\n\n  async queueTextCompletion(context: string, temperature: number, stop: string[], frequency_penalty: number, presence_penalty: number, max_tokens: number): Promise<string> {\n    await this.ensureInitialized();\n\n    return new Promise((resolve, reject) => {\n      this.messageQueue.push({\n        context,\n        temperature,\n        stop,\n        frequency_penalty: frequency_penalty ?? 1.0,\n        presence_penalty: presence_penalty ?? 1.0,\n        max_tokens,\n        useGrammar: false,\n        resolve,\n        reject,\n      });\n      this.processQueue();\n    });\n  }\n\n  private async processQueue() {\n    if (this.isProcessing || this.messageQueue.length === 0 || !this.modelInitialized) {\n      return;\n    }\n\n    this.isProcessing = true;\n\n    while (this.messageQueue.length > 0) {\n      const message = this.messageQueue.shift();\n      if (message) {\n        try {\n          const response = await this.getCompletionResponse(message.context, message.temperature, message.stop, message.frequency_penalty, message.presence_penalty, message.max_tokens, message.useGrammar);\n          message.resolve(response);\n        } catch (error) {\n          message.reject(error);\n        }\n      }\n    }\n\n    this.isProcessing = false;\n  }\n\n  async completion(prompt: string, runtime: IAgentRuntime): Promise<string> {\n    try {\n      await this.initialize(runtime);\n\n      if (runtime.modelProvider === ModelProviderName.OLLAMA) {\n        return await this.ollamaCompletion(prompt);\n      }\n\n      return await this.localCompletion(prompt);\n    } catch (error) {\n      elizaLogger.error(\"Error in completion:\", error);\n      throw error;\n    }\n  }\n\n  async embedding(text: string, runtime: IAgentRuntime): Promise<number[]> {\n    try {\n      await this.initialize(runtime);\n\n      if (runtime.modelProvider === ModelProviderName.OLLAMA) {\n        return await this.ollamaEmbedding(text);\n      }\n\n      return await this.localEmbedding(text);\n    } catch (error) {\n      elizaLogger.error(\"Error in embedding:\", error);\n      throw error;\n    }\n  }\n\n  private async getCompletionResponse(context: string, temperature: number, stop: string[], frequency_penalty: number, presence_penalty: number, max_tokens: number, useGrammar: boolean): Promise<any | string> {\n    context = context += \"\\nIMPORTANT: Escape any quotes in any string fields with a backslash so the JSON is valid.\";\n\n    const ollamaModel = process.env.OLLAMA_MODEL;\n    if (ollamaModel) {\n      const ollamaUrl = process.env.OLLAMA_SERVER_URL || \"http://localhost:11434\";\n      elizaLogger.info(`Using Ollama API at ${ollamaUrl} with model ${ollamaModel}`);\n\n      const response = await fetch(`${ollamaUrl}/api/generate`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({\n          model: ollamaModel,\n          prompt: context,\n          stream: false,\n          options: {\n            temperature,\n            stop,\n            frequency_penalty,\n            presence_penalty,\n            num_predict: max_tokens,\n          },\n        }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`Ollama request failed: ${response.statusText}`);\n      }\n\n      const result = await response.json();\n      return useGrammar ? { content: result.response } : result.response;\n    }\n\n    // Use local GGUF model\n    if (!this.sequence) {\n      throw new Error(\"Model not initialized.\");\n    }\n\n    const session = new LlamaChatSession({\n      contextSequence: this.sequence,\n    });\n\n    const wordsToPunishTokens = wordsToPunish.flatMap((word) => this.model!.tokenize(word));\n\n    const repeatPenalty: LlamaChatSessionRepeatPenalty = {\n      punishTokensFilter: () => wordsToPunishTokens,\n      penalty: 1.2,\n      frequencyPenalty: frequency_penalty,\n      presencePenalty: presence_penalty,\n    };\n\n    const response = await session.prompt(context, {\n      onTextChunk(chunk) {\n        // stream the response to the console as it's being generated\n        process.stdout.write(chunk);\n      },\n      temperature: Number(temperature),\n      repeatPenalty: repeatPenalty,\n    });\n\n    if (!response) {\n      throw new Error(\"Response is undefined\");\n    }\n\n    if (useGrammar) {\n      // extract everything between ```json and ```\n      let jsonString = response.match(/```json(.*?)```/s)?.[1].trim();\n      if (!jsonString) {\n        // try parsing response as JSON\n        try {\n          jsonString = JSON.stringify(JSON.parse(response));\n        } catch {\n          throw new Error(\"JSON string not found\");\n        }\n      }\n      try {\n        const parsedResponse = JSON.parse(jsonString);\n        if (!parsedResponse) {\n          throw new Error(\"Parsed response is undefined\");\n        }\n        await this.sequence.clearHistory();\n        return parsedResponse;\n      } catch (error) {\n        elizaLogger.error(\"Error parsing JSON:\", error);\n      }\n    } else {\n      await this.sequence.clearHistory();\n      return response;\n    }\n  }\n\n  async getEmbeddingResponse(input: string): Promise<number[] | undefined> {\n    const ollamaModel = process.env.OLLAMA_MODEL;\n    if (ollamaModel) {\n      const ollamaUrl = process.env.OLLAMA_SERVER_URL || \"http://localhost:11434\";\n      const embeddingModel = process.env.OLLAMA_EMBEDDING_MODEL || \"mxbai-embed-large\";\n      elizaLogger.info(`Using Ollama API for embeddings with model ${embeddingModel} (base: ${ollamaModel})`);\n\n      const response = await fetch(`${ollamaUrl}/api/embeddings`, {\n        method: \"POST\",\n        headers: { \"Content-Type\": \"application/json\" },\n        body: JSON.stringify({\n          model: embeddingModel,\n          prompt: input,\n        }),\n      });\n\n      if (!response.ok) {\n        throw new Error(`Ollama embeddings request failed: ${response.statusText}`);\n      }\n\n      const result = await response.json();\n      return result.embedding;\n    }\n\n    // Use local GGUF model\n    if (!this.sequence) {\n      throw new Error(\"Sequence not initialized\");\n    }\n\n    const ollamaUrl = process.env.OLLAMA_SERVER_URL || \"http://localhost:11434\";\n    const embeddingModel = process.env.OLLAMA_EMBEDDING_MODEL || \"mxbai-embed-large\";\n    elizaLogger.info(`Using Ollama API for embeddings with model ${embeddingModel} (base: ${this.ollamaModel})`);\n\n    const response = await fetch(`${ollamaUrl}/api/embeddings`, {\n      method: \"POST\",\n      headers: {\n        \"Content-Type\": \"application/json\",\n      },\n      body: JSON.stringify({\n        input: input,\n        model: embeddingModel,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Failed to get embedding: ${response.statusText}`);\n    }\n\n    const embedding = await response.json();\n    return embedding.vector;\n  }\n\n  private async ollamaCompletion(prompt: string): Promise<string> {\n    const ollamaModel = process.env.OLLAMA_MODEL;\n    const ollamaUrl = process.env.OLLAMA_SERVER_URL || \"http://localhost:11434\";\n    elizaLogger.info(`Using Ollama API at ${ollamaUrl} with model ${ollamaModel}`);\n\n    const response = await fetch(`${ollamaUrl}/api/generate`, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        model: ollamaModel,\n        prompt: prompt,\n        stream: false,\n        options: {\n          temperature: 0.7,\n          stop: [\"\\n\"],\n          frequency_penalty: 0.5,\n          presence_penalty: 0.5,\n          num_predict: 256,\n        },\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Ollama request failed: ${response.statusText}`);\n    }\n\n    const result = await response.json();\n    return result.response;\n  }\n\n  private async ollamaEmbedding(text: string): Promise<number[]> {\n    const ollamaModel = process.env.OLLAMA_MODEL;\n    const ollamaUrl = process.env.OLLAMA_SERVER_URL || \"http://localhost:11434\";\n    const embeddingModel = process.env.OLLAMA_EMBEDDING_MODEL || \"mxbai-embed-large\";\n    elizaLogger.info(`Using Ollama API for embeddings with model ${embeddingModel} (base: ${ollamaModel})`);\n\n    const response = await fetch(`${ollamaUrl}/api/embeddings`, {\n      method: \"POST\",\n      headers: { \"Content-Type\": \"application/json\" },\n      body: JSON.stringify({\n        model: embeddingModel,\n        prompt: text,\n      }),\n    });\n\n    if (!response.ok) {\n      throw new Error(`Ollama embeddings request failed: ${response.statusText}`);\n    }\n\n    const result = await response.json();\n    return result.embedding;\n  }\n\n  private async localCompletion(prompt: string): Promise<string> {\n    if (!this.sequence) {\n      throw new Error(\"Sequence not initialized\");\n    }\n\n    const tokens = this.model!.tokenize(prompt);\n\n    // tokenize the words to punish\n    const wordsToPunishTokens = wordsToPunish.flatMap((word) => this.model!.tokenize(word));\n\n    const repeatPenalty: LlamaContextSequenceRepeatPenalty = {\n      punishTokens: () => wordsToPunishTokens,\n      penalty: 1.2,\n      frequencyPenalty: 0.5,\n      presencePenalty: 0.5,\n    };\n\n    const responseTokens: Token[] = [];\n\n    for await (const token of this.sequence.evaluate(tokens, {\n      temperature: 0.7,\n      repeatPenalty: repeatPenalty,\n      yieldEogToken: false,\n    })) {\n      const current = this.model.detokenize([...responseTokens, token]);\n      if (current.includes(\"\\n\")) {\n        elizaLogger.info(\"Stop sequence found\");\n        break;\n      }\n\n      responseTokens.push(token);\n      process.stdout.write(this.model!.detokenize([token]));\n      if (responseTokens.length > 256) {\n        elizaLogger.info(\"Max tokens reached\");\n        break;\n      }\n    }\n\n    const response = this.model!.detokenize(responseTokens);\n\n    if (!response) {\n      throw new Error(\"Response is undefined\");\n    }\n\n    await this.sequence.clearHistory();\n    return response;\n  }\n\n  private async localEmbedding(text: string): Promise<number[]> {\n    if (!this.sequence) {\n      throw new Error(\"Sequence not initialized\");\n    }\n\n    const embeddingContext = await this.model.createEmbeddingContext();\n    const embedding = await embeddingContext.getEmbeddingFor(text);\n    return embedding?.vector ? [...embedding.vector] : undefined;\n  }\n}\n\nexport default LlamaService;\n","\nimport { describeImage } from \"./actions/describe-image\";\nimport { LlamaService } from \"./services/llama\";\n\nconst imagePlugin = {\n  name: \"default\",\n  description: \"Default plugin, with basic actions and evaluators\",\n  services: [new LlamaService() as any],\n  actions: [describeImage],\n}; \n\nexport default imagePlugin;\n"],"mappings":";AAAA;AAAA,EAMI;AAAA,EACA;AAAA,EAEA;AAAA,EACA;AAAA,EACA;AAAA,OAEG;;;ACbA,IAAM,0BAA0B;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;AAAA;;;ACAvC,SAAS,SAAS;AAEX,IAAM,2BAA2B,EAAE,OAAO;AAAA,EAC7C,cAAc,EAAE,OAAO,EAAE,IAAI,CAAC;AAClC,CAAC;AAIM,SAAS,qBAAqB,KAAyC;AAC1E,SAAO,yBAAyB,UAAU,GAAG,EAAE;AACnD;;;AFOO,IAAM,gBAAwB;AAAA,EACjC,MAAM;AAAA,EACN,SAAS,CAAC,oBAAoB,mBAAmB,eAAe;AAAA,EAChE,UAAU,OAAO,UAAyB,aAAqB;AAC3D,WAAO;AAAA,EACX;AAAA,EACA,aAAa;AAAA,EACb,SAAS,OACL,SACA,SACA,OACA,UACA,aACmB;AAEnB,UAAM,yBAAyB,eAAe;AAAA,MAC1C;AAAA,MACA,UAAU;AAAA,IACd,CAAC;AAED,UAAM,2BAA2B,MAAM,eAAe;AAAA,MAClD;AAAA,MACA,SAAS;AAAA,MACT,YAAY,WAAW;AAAA,MACvB,QAAQ;AAAA,MACR,MAAM,CAAC,IAAI;AAAA,IACf,CAAC;AAED,QACI,CAAC;AAAA,MACG,0BAA0B,UAAU;AAAA,IACxC,GACF;AACE,kBAAY,MAAM,kCAAkC;AACpD,aAAO;AAAA,IACX;AAEA,QAAI,eAAgB,0BAA0B,QACxC;AACN,oCAAiB;AAEjB,UAAM,EAAE,YAAY,IAAI,MAAM,QACzB,WAAqC,YAAY,iBAAiB,EAClE,cAAc,YAAY;AAE/B,YAAQ,eAAe,aAAa;AAAA,MAChC,QAAQ,QAAQ;AAAA,MAChB,SAAS,QAAQ;AAAA,MACjB,QAAQ,QAAQ;AAAA,MAChB,SAAS;AAAA,QACL,MAAM;AAAA,MACV;AAAA,IACJ,CAAC;AAED,aAAS;AAAA,MACL,MAAM;AAAA,IACV,CAAC;AAED,WAAO;AAAA,EACX;AAAA,EACA,UAAU;AAAA,IACN;AAAA,MACI;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,UACN,QAAQ;AAAA,QACZ;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,IACJ;AAAA,IACA;AAAA,MACI;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,UACN,QAAQ;AAAA,QACZ;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,IACJ;AAAA,IACA;AAAA,MACI;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,UACN,QAAQ;AAAA,QACZ;AAAA,MACJ;AAAA,MACA;AAAA,QACI,MAAM;AAAA,QACN,SAAS;AAAA,UACL,MAAM;AAAA,QACV;AAAA,MACJ;AAAA,IACJ;AAAA,EACJ;AACJ;;;AG9IA,SAAS,eAAAA,cAAiC,eAAAC,cAAa,yBAAyB;AAChF,SAAS,eAAe;AACxB,OAAO,QAAQ;AACf,OAAO,WAAW;AAClB,SAA8B,UAAsB,kBAA4I,8BAA2D;AAC3P,OAAO,UAAU;AACjB,OAAO,QAAQ;AACf,SAAS,qBAAqB;AAE9B,IAAM,gBAAgB;AAAA,EACpB;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AAAA,EACA;AACF;AAEA,IAAM,YAAY,KAAK,QAAQ,cAAc,YAAY,GAAG,CAAC;AAE7D,IAAM,oBAUD;AAAA,EACH,MAAM;AAAA,EACN,YAAY;AAAA,IACV,MAAM;AAAA,MACJ,MAAM;AAAA,IACR;AAAA,IACA,SAAS;AAAA,MACP,MAAM;AAAA,IACR;AAAA,EACF;AACF;AAcO,IAAM,eAAN,cAA2B,QAAQ;AAAA,EAiBxC,cAAc;AACZ,UAAM;AARR,SAAQ,eAAgC,CAAC;AACzC,SAAQ,eAAe;AACvB,SAAQ,mBAAmB;AAOzB,SAAK,QAAQ;AACb,SAAK,QAAQ;AACb,SAAK,WAAW;AAAA,EAElB;AAAA,EAEA,MAAM,WAAW,SAAuC;AACtD,UAAM,YAAY;AAClB,IAAAD,aAAY,KAAK,8BAA8B;AAC/C,SAAK,UAAU;AACf,SAAK,YAAY,KAAK,KAAK,QAAQ,WAAW,iBAAiB,EAAE,KAAK,KAAK,MAAM,SAAS;AAC1F,SAAK,cAAc,QAAQ,WAAW,cAAc;AAAA,EACtD;AAAA,EAEA,MAAc,oBAAoB;AAChC,QAAI,CAAC,KAAK,kBAAkB;AAC1B,MAAAA,aAAY,KAAK,mDAAmD;AACpE,YAAM,KAAK,gBAAgB;AAAA,IAC7B,OAAO;AACL,MAAAA,aAAY,KAAK,2BAA2B;AAAA,IAC9C;AAAA,EACF;AAAA,EAEA,MAAM,kBAAkB;AACtB,QAAI;AACF,MAAAA,aAAY,KAAK,wBAAwB;AACzC,YAAM,KAAK,WAAW;AAEtB,YAAM,aAAa,MAAM,GAAG,SAAS;AACrC,YAAM,UAAU,WAAW,YAAY,KAAK,CAAC,eAAe,WAAW,OAAO,YAAY,EAAE,SAAS,QAAQ,CAAC;AAE9G,UAAI,SAAS;AACX,QAAAA,aAAY,KAAK,qDAAqD;AAAA,MACxE,OAAO;AACL,QAAAA,aAAY,KAAK,8DAA8D;AAAA,MACjF;AAEA,MAAAA,aAAY,KAAK,gCAAgC;AACjD,WAAK,QAAQ,MAAM,SAAS;AAAA,QAC1B,KAAK,UAAU,SAAS;AAAA,MAC1B,CAAC;AAED,MAAAA,aAAY,KAAK,iCAAiC;AAClD,YAAM,UAAU,IAAI,uBAAuB,KAAK,OAAO,iBAAmC;AAC1F,WAAK,UAAU;AAEf,MAAAA,aAAY,KAAK,kBAAkB;AACnC,WAAK,QAAQ,MAAM,KAAK,MAAM,UAAU;AAAA,QACtC,WAAW,KAAK;AAAA,MAClB,CAAC;AAED,MAAAA,aAAY,KAAK,kCAAkC;AACnD,WAAK,MAAM,MAAM,KAAK,MAAM,cAAc,EAAE,aAAa,KAAK,CAAC;AAC/D,WAAK,WAAW,KAAK,IAAI,YAAY;AAErC,WAAK,mBAAmB;AACxB,MAAAA,aAAY,QAAQ,+BAA+B;AACnD,WAAK,aAAa;AAAA,IACpB,SAAS,OAAO;AACd,MAAAA,aAAY,MAAM,6DAA6D,KAAK;AACpF,UAAI;AACF,QAAAA,aAAY,KAAK,+CAA+C;AAChE,cAAM,KAAK,YAAY;AACvB,cAAM,KAAK,gBAAgB;AAAA,MAC7B,SAAS,YAAY;AACnB,QAAAA,aAAY,MAAM,mCAAmC,UAAU;AAC/D,cAAM,IAAI,MAAM,4CAA4C,WAAW,OAAO,EAAE;AAAA,MAClF;AAAA,IACF;AAAA,EACF;AAAA,EAEA,MAAM,aAAa;AACjB,QAAI,CAAC,GAAG,WAAW,KAAK,SAAS,GAAG;AAClC,MAAAA,aAAY,KAAK,4CAA4C;AAC7D,YAAM,IAAI,QAAc,CAAC,SAAS,WAAW;AAC3C,cAAM,OAAO,GAAG,kBAAkB,KAAK,SAAS;AAChD,YAAI,iBAAiB;AACrB,YAAI,YAAY;AAEhB,cAAM,gBAAgB,CAAC,QAAgB;AACrC,gBACG,IAAI,KAAK,CAAC,aAAa;AACtB,gBAAI,SAAS,cAAc,OAAO,SAAS,aAAa,OAAO,SAAS,QAAQ,UAAU;AACxF,cAAAA,aAAY,KAAK,0BAA0B,SAAS,QAAQ,QAAQ,EAAE;AACtE,4BAAc,SAAS,QAAQ,QAAQ;AACvC;AAAA,YACF;AAEA,gBAAI,SAAS,eAAe,KAAK;AAC/B,qBAAO,IAAI,MAAM,kCAAkC,SAAS,UAAU,EAAE,CAAC;AACzE;AAAA,YACF;AAEA,wBAAY,OAAO,SAAS,SAAS,QAAQ,gBAAgB,KAAK,KAAK,EAAE;AACzE,YAAAA,aAAY,KAAK,oDAAoD;AACrE,YAAAA,aAAY,KAAK,sBAAsB,KAAK,SAAS,EAAE;AACvD,YAAAA,aAAY,KAAK,gBAAgB,YAAY,OAAO,MAAM,QAAQ,CAAC,CAAC,KAAK;AAEzE,qBAAS,KAAK,IAAI;AAElB,gBAAI,iBAAiB;AACrB,qBAAS,GAAG,QAAQ,CAAC,UAAU;AAC7B,gCAAkB,MAAM;AACxB,oBAAM,WAAW,YAAY,KAAM,iBAAiB,YAAa,KAAK,QAAQ,CAAC,IAAI;AACnF,oBAAM,OAAO,IAAI,OAAO,KAAK,MAAM,OAAO,QAAQ,IAAI,CAAC,CAAC;AACxD,+BAAiB,uBAAuB,KAAK,OAAO,IAAI,GAAG,CAAC,KAAK,QAAQ;AACzE,cAAAA,aAAY,SAAS,cAAc;AAAA,YACrC,CAAC;AAED,iBAAK,GAAG,UAAU,MAAM;AACtB,mBAAK,MAAM;AACX,cAAAA,aAAY,SAAS,EAAE;AACvB,cAAAA,aAAY,QAAQ,yBAAyB;AAC7C,sBAAQ;AAAA,YACV,CAAC;AAED,qBAAS,GAAG,SAAS,CAAC,UAAU;AAC9B,iBAAG,OAAO,KAAK,WAAW,MAAM;AAAA,cAAC,CAAC;AAClC,qBAAO,IAAI,MAAM,0BAA0B,MAAM,OAAO,EAAE,CAAC;AAAA,YAC7D,CAAC;AAAA,UACH,CAAC,EACA,GAAG,SAAS,CAAC,UAAU;AACtB,eAAG,OAAO,KAAK,WAAW,MAAM;AAAA,YAAC,CAAC;AAClC,mBAAO,IAAI,MAAM,kCAAkC,MAAM,OAAO,EAAE,CAAC;AAAA,UACrE,CAAC;AAAA,QACL;AAEA,sBAAc,KAAK,QAAQ;AAE3B,aAAK,GAAG,SAAS,CAAC,QAAQ;AACxB,aAAG,OAAO,KAAK,WAAW,MAAM;AAAA,UAAC,CAAC;AAClC,kBAAQ,MAAM,qBAAqB,IAAI,OAAO;AAC9C,iBAAO,GAAG;AAAA,QACZ,CAAC;AAAA,MACH,CAAC;AAAA,IACH,OAAO;AACL,MAAAA,aAAY,KAAK,uBAAuB;AAAA,IAC1C;AAAA,EACF;AAAA,EAEA,MAAM,cAAc;AAClB,QAAI,GAAG,WAAW,KAAK,SAAS,GAAG;AACjC,SAAG,WAAW,KAAK,SAAS;AAAA,IAC9B;AAAA,EACF;AAAA,EAEA,MAAM,uBAAuB,SAAiB,aAAqB,MAAgB,mBAA2B,kBAA0B,YAAkC;AACxK,UAAM,KAAK,kBAAkB;AAC7B,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,WAAK,aAAa,KAAK;AAAA,QACrB;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA;AAAA,QACA,YAAY;AAAA,QACZ;AAAA,QACA;AAAA,MACF,CAAC;AACD,WAAK,aAAa;AAAA,IACpB,CAAC;AAAA,EACH;AAAA,EAEA,MAAM,oBAAoB,SAAiB,aAAqB,MAAgB,mBAA2B,kBAA0B,YAAqC;AACxK,UAAM,KAAK,kBAAkB;AAE7B,WAAO,IAAI,QAAQ,CAAC,SAAS,WAAW;AACtC,WAAK,aAAa,KAAK;AAAA,QACrB;AAAA,QACA;AAAA,QACA;AAAA,QACA,mBAAmB,qBAAqB;AAAA,QACxC,kBAAkB,oBAAoB;AAAA,QACtC;AAAA,QACA,YAAY;AAAA,QACZ;AAAA,QACA;AAAA,MACF,CAAC;AACD,WAAK,aAAa;AAAA,IACpB,CAAC;AAAA,EACH;AAAA,EAEA,MAAc,eAAe;AAC3B,QAAI,KAAK,gBAAgB,KAAK,aAAa,WAAW,KAAK,CAAC,KAAK,kBAAkB;AACjF;AAAA,IACF;AAEA,SAAK,eAAe;AAEpB,WAAO,KAAK,aAAa,SAAS,GAAG;AACnC,YAAM,UAAU,KAAK,aAAa,MAAM;AACxC,UAAI,SAAS;AACX,YAAI;AACF,gBAAM,WAAW,MAAM,KAAK,sBAAsB,QAAQ,SAAS,QAAQ,aAAa,QAAQ,MAAM,QAAQ,mBAAmB,QAAQ,kBAAkB,QAAQ,YAAY,QAAQ,UAAU;AACjM,kBAAQ,QAAQ,QAAQ;AAAA,QAC1B,SAAS,OAAO;AACd,kBAAQ,OAAO,KAAK;AAAA,QACtB;AAAA,MACF;AAAA,IACF;AAEA,SAAK,eAAe;AAAA,EACtB;AAAA,EAEA,MAAM,WAAW,QAAgB,SAAyC;AACxE,QAAI;AACF,YAAM,KAAK,WAAW,OAAO;AAE7B,UAAI,QAAQ,kBAAkB,kBAAkB,QAAQ;AACtD,eAAO,MAAM,KAAK,iBAAiB,MAAM;AAAA,MAC3C;AAEA,aAAO,MAAM,KAAK,gBAAgB,MAAM;AAAA,IAC1C,SAAS,OAAO;AACd,MAAAA,aAAY,MAAM,wBAAwB,KAAK;AAC/C,YAAM;AAAA,IACR;AAAA,EACF;AAAA,EAEA,MAAM,UAAU,MAAc,SAA2C;AACvE,QAAI;AACF,YAAM,KAAK,WAAW,OAAO;AAE7B,UAAI,QAAQ,kBAAkB,kBAAkB,QAAQ;AACtD,eAAO,MAAM,KAAK,gBAAgB,IAAI;AAAA,MACxC;AAEA,aAAO,MAAM,KAAK,eAAe,IAAI;AAAA,IACvC,SAAS,OAAO;AACd,MAAAA,aAAY,MAAM,uBAAuB,KAAK;AAC9C,YAAM;AAAA,IACR;AAAA,EACF;AAAA,EAEA,MAAc,sBAAsB,SAAiB,aAAqB,MAAgB,mBAA2B,kBAA0B,YAAoB,YAA4C;AAC7M,cAAU,WAAW;AAErB,UAAM,cAAc,QAAQ,IAAI;AAChC,QAAI,aAAa;AACf,YAAM,YAAY,QAAQ,IAAI,qBAAqB;AACnD,MAAAA,aAAY,KAAK,uBAAuB,SAAS,eAAe,WAAW,EAAE;AAE7E,YAAME,YAAW,MAAM,MAAM,GAAG,SAAS,iBAAiB;AAAA,QACxD,QAAQ;AAAA,QACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,QAC9C,MAAM,KAAK,UAAU;AAAA,UACnB,OAAO;AAAA,UACP,QAAQ;AAAA,UACR,QAAQ;AAAA,UACR,SAAS;AAAA,YACP;AAAA,YACA;AAAA,YACA;AAAA,YACA;AAAA,YACA,aAAa;AAAA,UACf;AAAA,QACF,CAAC;AAAA,MACH,CAAC;AAED,UAAI,CAACA,UAAS,IAAI;AAChB,cAAM,IAAI,MAAM,0BAA0BA,UAAS,UAAU,EAAE;AAAA,MACjE;AAEA,YAAM,SAAS,MAAMA,UAAS,KAAK;AACnC,aAAO,aAAa,EAAE,SAAS,OAAO,SAAS,IAAI,OAAO;AAAA,IAC5D;AAGA,QAAI,CAAC,KAAK,UAAU;AAClB,YAAM,IAAI,MAAM,wBAAwB;AAAA,IAC1C;AAEA,UAAM,UAAU,IAAI,iBAAiB;AAAA,MACnC,iBAAiB,KAAK;AAAA,IACxB,CAAC;AAED,UAAM,sBAAsB,cAAc,QAAQ,CAAC,SAAS,KAAK,MAAO,SAAS,IAAI,CAAC;AAEtF,UAAM,gBAA+C;AAAA,MACnD,oBAAoB,MAAM;AAAA,MAC1B,SAAS;AAAA,MACT,kBAAkB;AAAA,MAClB,iBAAiB;AAAA,IACnB;AAEA,UAAM,WAAW,MAAM,QAAQ,OAAO,SAAS;AAAA,MAC7C,YAAY,OAAO;AAEjB,gBAAQ,OAAO,MAAM,KAAK;AAAA,MAC5B;AAAA,MACA,aAAa,OAAO,WAAW;AAAA,MAC/B;AAAA,IACF,CAAC;AAED,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,uBAAuB;AAAA,IACzC;AAEA,QAAI,YAAY;AAEd,UAAI,aAAa,SAAS,MAAM,kBAAkB,IAAI,CAAC,EAAE,KAAK;AAC9D,UAAI,CAAC,YAAY;AAEf,YAAI;AACF,uBAAa,KAAK,UAAU,KAAK,MAAM,QAAQ,CAAC;AAAA,QAClD,QAAQ;AACN,gBAAM,IAAI,MAAM,uBAAuB;AAAA,QACzC;AAAA,MACF;AACA,UAAI;AACF,cAAM,iBAAiB,KAAK,MAAM,UAAU;AAC5C,YAAI,CAAC,gBAAgB;AACnB,gBAAM,IAAI,MAAM,8BAA8B;AAAA,QAChD;AACA,cAAM,KAAK,SAAS,aAAa;AACjC,eAAO;AAAA,MACT,SAAS,OAAO;AACd,QAAAF,aAAY,MAAM,uBAAuB,KAAK;AAAA,MAChD;AAAA,IACF,OAAO;AACL,YAAM,KAAK,SAAS,aAAa;AACjC,aAAO;AAAA,IACT;AAAA,EACF;AAAA,EAEA,MAAM,qBAAqB,OAA8C;AACvE,UAAM,cAAc,QAAQ,IAAI;AAChC,QAAI,aAAa;AACf,YAAMG,aAAY,QAAQ,IAAI,qBAAqB;AACnD,YAAMC,kBAAiB,QAAQ,IAAI,0BAA0B;AAC7D,MAAAJ,aAAY,KAAK,8CAA8CI,eAAc,WAAW,WAAW,GAAG;AAEtG,YAAMF,YAAW,MAAM,MAAM,GAAGC,UAAS,mBAAmB;AAAA,QAC1D,QAAQ;AAAA,QACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,QAC9C,MAAM,KAAK,UAAU;AAAA,UACnB,OAAOC;AAAA,UACP,QAAQ;AAAA,QACV,CAAC;AAAA,MACH,CAAC;AAED,UAAI,CAACF,UAAS,IAAI;AAChB,cAAM,IAAI,MAAM,qCAAqCA,UAAS,UAAU,EAAE;AAAA,MAC5E;AAEA,YAAM,SAAS,MAAMA,UAAS,KAAK;AACnC,aAAO,OAAO;AAAA,IAChB;AAGA,QAAI,CAAC,KAAK,UAAU;AAClB,YAAM,IAAI,MAAM,0BAA0B;AAAA,IAC5C;AAEA,UAAM,YAAY,QAAQ,IAAI,qBAAqB;AACnD,UAAM,iBAAiB,QAAQ,IAAI,0BAA0B;AAC7D,IAAAF,aAAY,KAAK,8CAA8C,cAAc,WAAW,KAAK,WAAW,GAAG;AAE3G,UAAM,WAAW,MAAM,MAAM,GAAG,SAAS,mBAAmB;AAAA,MAC1D,QAAQ;AAAA,MACR,SAAS;AAAA,QACP,gBAAgB;AAAA,MAClB;AAAA,MACA,MAAM,KAAK,UAAU;AAAA,QACnB;AAAA,QACA,OAAO;AAAA,MACT,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,4BAA4B,SAAS,UAAU,EAAE;AAAA,IACnE;AAEA,UAAM,YAAY,MAAM,SAAS,KAAK;AACtC,WAAO,UAAU;AAAA,EACnB;AAAA,EAEA,MAAc,iBAAiB,QAAiC;AAC9D,UAAM,cAAc,QAAQ,IAAI;AAChC,UAAM,YAAY,QAAQ,IAAI,qBAAqB;AACnD,IAAAA,aAAY,KAAK,uBAAuB,SAAS,eAAe,WAAW,EAAE;AAE7E,UAAM,WAAW,MAAM,MAAM,GAAG,SAAS,iBAAiB;AAAA,MACxD,QAAQ;AAAA,MACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,MAC9C,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO;AAAA,QACP;AAAA,QACA,QAAQ;AAAA,QACR,SAAS;AAAA,UACP,aAAa;AAAA,UACb,MAAM,CAAC,IAAI;AAAA,UACX,mBAAmB;AAAA,UACnB,kBAAkB;AAAA,UAClB,aAAa;AAAA,QACf;AAAA,MACF,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,0BAA0B,SAAS,UAAU,EAAE;AAAA,IACjE;AAEA,UAAM,SAAS,MAAM,SAAS,KAAK;AACnC,WAAO,OAAO;AAAA,EAChB;AAAA,EAEA,MAAc,gBAAgB,MAAiC;AAC7D,UAAM,cAAc,QAAQ,IAAI;AAChC,UAAM,YAAY,QAAQ,IAAI,qBAAqB;AACnD,UAAM,iBAAiB,QAAQ,IAAI,0BAA0B;AAC7D,IAAAA,aAAY,KAAK,8CAA8C,cAAc,WAAW,WAAW,GAAG;AAEtG,UAAM,WAAW,MAAM,MAAM,GAAG,SAAS,mBAAmB;AAAA,MAC1D,QAAQ;AAAA,MACR,SAAS,EAAE,gBAAgB,mBAAmB;AAAA,MAC9C,MAAM,KAAK,UAAU;AAAA,QACnB,OAAO;AAAA,QACP,QAAQ;AAAA,MACV,CAAC;AAAA,IACH,CAAC;AAED,QAAI,CAAC,SAAS,IAAI;AAChB,YAAM,IAAI,MAAM,qCAAqC,SAAS,UAAU,EAAE;AAAA,IAC5E;AAEA,UAAM,SAAS,MAAM,SAAS,KAAK;AACnC,WAAO,OAAO;AAAA,EAChB;AAAA,EAEA,MAAc,gBAAgB,QAAiC;AAC7D,QAAI,CAAC,KAAK,UAAU;AAClB,YAAM,IAAI,MAAM,0BAA0B;AAAA,IAC5C;AAEA,UAAM,SAAS,KAAK,MAAO,SAAS,MAAM;AAG1C,UAAM,sBAAsB,cAAc,QAAQ,CAAC,SAAS,KAAK,MAAO,SAAS,IAAI,CAAC;AAEtF,UAAM,gBAAmD;AAAA,MACvD,cAAc,MAAM;AAAA,MACpB,SAAS;AAAA,MACT,kBAAkB;AAAA,MAClB,iBAAiB;AAAA,IACnB;AAEA,UAAM,iBAA0B,CAAC;AAEjC,qBAAiB,SAAS,KAAK,SAAS,SAAS,QAAQ;AAAA,MACvD,aAAa;AAAA,MACb;AAAA,MACA,eAAe;AAAA,IACjB,CAAC,GAAG;AACF,YAAM,UAAU,KAAK,MAAM,WAAW,CAAC,GAAG,gBAAgB,KAAK,CAAC;AAChE,UAAI,QAAQ,SAAS,IAAI,GAAG;AAC1B,QAAAA,aAAY,KAAK,qBAAqB;AACtC;AAAA,MACF;AAEA,qBAAe,KAAK,KAAK;AACzB,cAAQ,OAAO,MAAM,KAAK,MAAO,WAAW,CAAC,KAAK,CAAC,CAAC;AACpD,UAAI,eAAe,SAAS,KAAK;AAC/B,QAAAA,aAAY,KAAK,oBAAoB;AACrC;AAAA,MACF;AAAA,IACF;AAEA,UAAM,WAAW,KAAK,MAAO,WAAW,cAAc;AAEtD,QAAI,CAAC,UAAU;AACb,YAAM,IAAI,MAAM,uBAAuB;AAAA,IACzC;AAEA,UAAM,KAAK,SAAS,aAAa;AACjC,WAAO;AAAA,EACT;AAAA,EAEA,MAAc,eAAe,MAAiC;AAC5D,QAAI,CAAC,KAAK,UAAU;AAClB,YAAM,IAAI,MAAM,0BAA0B;AAAA,IAC5C;AAEA,UAAM,mBAAmB,MAAM,KAAK,MAAM,uBAAuB;AACjE,UAAM,YAAY,MAAM,iBAAiB,gBAAgB,IAAI;AAC7D,WAAO,WAAW,SAAS,CAAC,GAAG,UAAU,MAAM,IAAI;AAAA,EACrD;AACF;AA3fa,aAeJ,cAA2BC,aAAY;;;AC/JhD,IAAM,cAAc;AAAA,EAClB,MAAM;AAAA,EACN,aAAa;AAAA,EACb,UAAU,CAAC,IAAI,aAAa,CAAQ;AAAA,EACpC,SAAS,CAAC,aAAa;AACzB;AAEA,IAAO,gBAAQ;","names":["elizaLogger","ServiceType","response","ollamaUrl","embeddingModel"]}